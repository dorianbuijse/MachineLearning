{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "# Graded Lab Assignment: Logistic Regression (10 points)\n",
    "\n",
    "In this assignment you will classify hand-written digits using logistic regression.\n",
    "\n",
    "The assignment follows Andrew Ng's explanation of Logistic Regression and (re)watching his videos could be useful (Week 3)\n",
    "\n",
    "Publish your notebook (ipynb file) to your Machine Learning repository on Github ON TIME. We will check the last commit on the day of the deadline. \n",
    "\n",
    "### Deadline Tuesday, October 10th, 23:59\n",
    "\n",
    "Do not hand in any other files, the Notebook should contain all your answers.\n",
    "\n",
    "The points for the assignment are distributed as follows:\n",
    "* The implementation\n",
    "    - prediction_function (0.5)\n",
    "    - cost_function (1)\n",
    "    - compute_gradient (1)\n",
    "    - correct double for loop (2)\n",
    "    - correct stop condition (0.5)\n",
    "    - preventing overfitting (1)\n",
    "    - systematically choosing learning rate (1) \n",
    "    - best score analysis (1)\n",
    "    - summary (1)\n",
    "    - comments (0.5)\n",
    "    - code (0.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn is a toolkit that has several datasets built in. You first need to install the toolkit: http://scikit-learn.org/stable/install.html\n",
    "\n",
    "The MNIST dataset that you will be using for this assignment contains images of hand-written digits that are only 8 by 8 pixels, which means the algorithm (logistic regression) should run on every computer.\n",
    "\n",
    "The code in the cell below shows how to work with the digits dataset and  how to visualize it. As you can see the numbers are not very clear in 8x8 pixels images, this means we cannot expect our logistic regression will have a very high classification score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xc566fd0>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAA/xJREFUeJzt3VFNI1EAhtGZDUIqAQnFARKKExxgAZwUB+MAJICCuw5g\nyba35cs5z5P5hyZf7gvJXccYC9D059IfAJyPwCFM4BAmcAgTOIQJHMIEDmEChzCBQ9jNOV66rqt/\njzuBbdumbR0Oh2lbM/+usjHG+t0zTnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEC\nhzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziEneXqoqqZ1/ssy7LsdrtpW64T\nanKCQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAm\ncAgTOIQJHMIEDmEChzCBQ5jAIczVRT8w++qix8fHqXv0OMEhTOAQJnAIEziECRzCBA5hAocwgUOY\nwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIW8cY\np3/pup7+pVfgHL/VV15eXqZtbds2bev5+Xna1sfHx7St2cYY63fPOMEhTOAQJnAIEziECRzCBA5h\nAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQ\ndnPpD/hfh8Ph0p9wNrvdbtrW7e3ttK2np6dpWw8PD9O2lmXutUz/wgkOYQKHMIFDmMAhTOAQJnAI\nEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKH\nsHWMcfqXruvpX3oFzvFbfeXu7m7a1vF4nLY187qp2Vdb7ff7aVtjjPW7Z5zgECZwCBM4hAkcwgQO\nYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzg\nECZwCLu59Af8Jq+vr1P3Zl678/7+Pm3r/v5+2ta2bdO2rpETHMIEDmEChzCBQ5jAIUzgECZwCBM4\nhAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzB3\nk/3AzDu1lmVZjsfjtK23t7dpW5+fn9O29vv9tK1r5ASHMIFDmMAhTOAQJnAIEziECRzCBA5hAocw\ngUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFD2DrGuPQ3AGfi\nBIcwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQ\nJnAIEziECRzC/gL/K1UIMe+YtAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa701ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits(n_class=10)\n",
    "\n",
    "#Create two rows with numbers\n",
    "firstrow = np.hstack(digits.images[:,:,:])\n",
    "secondrow = np.hstack(digits.images[5:10,:,:])\n",
    "thirdrow = np.hstack(digits.images[10:15,:,:])\n",
    "\n",
    "plt.gray()\n",
    "plt.axis('off')\n",
    "\n",
    "#Show both rows at the same time\n",
    "#plt.imshow(np.vstack((firstrow,secondrow, thirdrow)))\n",
    "\n",
    "plt.imshow(hstack(digits.images[1503:1504,:,:]))\n",
    "\n",
    "#print \"The numbers shown are: \\n\", np.vstack((digits.target[:5], digits.target[5:10], digits.target[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The implementation\n",
    "\n",
    "You have to implement the next three functions and fill in the body of the loop in order to create a correct implementation of logistic regression. Don't change the definitions of the functions and input parameters.\n",
    "\n",
    "(1) Make sure that you do not overfit by keeping track of the score on the test set and implementing a correct stop condition. \n",
    "(2) Systematically pick a learning rate alpha that makes sure the algorithm learns in a smooth and stable manner (show how you do it). \n",
    "(3) Plot how your score on the test set improves over time. My best score was about 85% correct!\n",
    "(4) Write a short summary of what you have done (and why) to accomplish steps (1), (2) and (3).\n",
    "(5) Make sure to comment your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make a prediction function h\n",
    "def hpred(x,theta):\n",
    "    # theta = Vector (64, 10), x = Vector (64,n) -> Vector (10,n)\n",
    "    x_predict = (1 / (1 + (e**-dot(theta.T,x))))\n",
    "    return x_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use the output of that function to compute the cost function J:\n",
    "\n",
    "def cost_function(x_predict,y):\n",
    "    # Vector (10,1), Vector (10,1) -> Vector (10, 1)\n",
    "    # number for each class\n",
    "    #for two classes, since eacht number (n) is classified as n or the others, this cost function works\n",
    "    cost = y * log(x_predict) + (1 - y) * log(1 - x_predict)\n",
    "    J = - sum(cost) / size(x_predict)\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a function that returns the gradient values, given h (x_predict), y and x:\n",
    "def compute_gradient(x_predict, y, x):\n",
    "    # Vector (10,1), Vector(10,1), Vector(64, 1) -> Vector (64, 10)\n",
    "        dtheta = (outer((x_predict - y), x) / size(x)).T\n",
    "        return dtheta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the trained model on our train set:\n",
      "98.4\n",
      "The accuracy of the trained model on our test set:\n",
      "89.5622895623\n"
     ]
    }
   ],
   "source": [
    "#Choose a suitable learning rate\n",
    "alpha = 0.0003\n",
    "iterations = 1600\n",
    "theta = zeros((64,10)) # 1 theta per pixel, 64 pixels, for 10 different classes\n",
    "\n",
    "\n",
    "#print(theta.)\n",
    "#It is important to check that you're not overfitting by testing your prediction on a testset\n",
    "x = reshape(digits.images[:1500],(1500,64))\n",
    "\n",
    "#Training Theta:\n",
    "for i in range(iterations): \n",
    "    for j in range(x.shape[0]): # 1500 examples\n",
    "                x_predict = hpred(x[j,:],theta)\n",
    "                y = np.zeros(10)\n",
    "                y[target[j]] = 1\n",
    "                cost = cost_function(x_predict,y)\n",
    "                dtheta = compute_gradient(x_predict, y, x[j, :])\n",
    "                theta -= alpha * dtheta\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "#Using Theta to test:\n",
    "\n",
    "target = digits.target[:1500]\n",
    "target_test = digits.target[1500:]\n",
    "x_test = reshape(digits.images[1500:],(297,64))\n",
    "\n",
    "def mainfnct(x, theta):\n",
    "    #for a new image input, find highest probability and return the index\n",
    "    return argmax(hpred(x.T, theta))\n",
    "\n",
    "\n",
    "trainerror = 0\n",
    "for k in range(x.shape[0]):\n",
    "    if mainfnct(x[k], theta) != target[k]: #If the guess is not equal to the actual number, add 1 to error\n",
    "        trainerror += 1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(\"The accuracy of the trained model on our train set:\")\n",
    "print((1- float(trainerror)/x.shape[0]) * 100)\n",
    "\n",
    "testerror = 0\n",
    "for k in range(x_test.shape[0]):\n",
    "    if mainfnct(x_test[k], theta) != target_test[k]: #If the guess is not equal to the actual number, add 1 to error\n",
    "        testerror += 1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(\"The accuracy of the trained model on our test set:\")\n",
    "print((1- float(testerror)/x_test.shape[0]) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "The function \"mainfnct is written to read an [64x1] matrix' containing grayscale values that represent the 8x8 images of 0-9 numbers, and returns the value of the numbers with a 90.24% accuracy.\n",
    "\n",
    "#### How it works:\n",
    "This model is trained on 1500 images represented by 64 pixels in a 1x64 array. A [64,10] matrix $\\theta$ is constructed, and through **n** itterations of the 1500 matrices (**x**) all 64 pixels of the 10 numbers (0-9) gets a $\\theta_i$ value.\n",
    "\n",
    "After the matrix $\\theta$ is trained, $\\theta$ gets used in the prediction function (**hpred**) represented by the mathematic equation:\n",
    "$h_{\\theta}(x)=\\frac{1}{1 + e^{-\\theta^T \\cdot x}}$ which returns a probability vector of dimensions [10,1] where probability i represents the probability *x* is the matrix belonging to the $i$-th number.\n",
    "\n",
    "The function **cost_function** was created for the exercise but since there seems to be barely any under or over fitting I felt it was unnecessary to test it.\n",
    "\n",
    "the function **compute_gradient** is one part of the gradient descent method used to optimize $\\theta$. Depending on the size of $\\alpha$ and the itterations in the double forloop, theta (preferably) converges to the optimal $\\theta$.\n",
    "\n",
    "**mainfnct** takes a [64x1] matrix and the final $\\theta$ and calculates the probability of all 10 numbers throuhg *hpred*. afterwards it returns the index of the highest probability in vector.\n",
    "\n",
    "To figure out the training- and testerror I used two loops which itterate over the length of the arrays (x, x_train resp.).\n",
    "Each loop it evaluates an input matrix [64x1] and returns a guess using **mainfnct**, after which it is compared to the real value. If **mainfnct** is incorrect it adds one to the error tally. After the loops the results are printed:  $E = (1 - \\frac{error}{number loops}) * 100 $\n",
    "\n",
    "**Note:** I chose to remove the while statement with the **stopcondition** from the forloops as it messed with my matrix computations, and it did not seem necessary. This also means that my cost function is not really used in any functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Varying the amount of itterations (**n**) and/or $\\alpha$ while training $\\theta$ changes the efficiency of the model.\n",
    "A small $\\alpha$ converges slowly, but does not overshoot, so a low $\\alpha$ and a higher *n* produce the most reliable model.\n",
    "\n",
    "I found that $\\alpha = 0.001$ gave me the best results.\n",
    "Paired with $n = 800$ it gave me a test accuracy of **90.24%** \n",
    "\n",
    "| Itterations:  |  25 |  100| 400 | 800  |1600|\n",
    "|---|---|---|---|---|---|\n",
    "| trained model  | 95.33  | 97.33  |  98.13 |  98.66 | 99.26  |\n",
    "|  tested model | 87.88  | 88.55  | 89.56  | **90.24**  | 89.90  |\n",
    "<p style=\"text-align: center;\" markdown=\"8\">Figure 1: different number of itterations at $\\alpha$ = 0.001</p>\n",
    "\n",
    "\n",
    "bigger $\\alpha$ requires less itterations, so I also tested some other combinations such as:\n",
    "* $\\alpha = 0.1, n = 10$ ---> trainerror: 92.67, testerror: 80.13\n",
    "* $\\alpha = 0.1, n = 100$ ---> trainerror: 97.20, testerror: 83.83\n",
    "* $\\alpha = 0.1, n = 800$ ---> trainerror: 98.12, testerror: 85.86\n",
    "* $\\alpha = 0.03, n = 800$ ---> trainerror: 99.27, testerror: 87.54\n",
    "* $\\alpha = 0.01, n = 800$ ---> trainerror: 99.53, testerror: 87.54\n",
    "* $\\alpha = 0.003, n = 800$ ---> trainerror: 99.33, testerror: 89.56\n",
    "* $\\alpha = 0.001, n = 800$ ---> trainerror: 98.66, testerror: **90.24**\n",
    "* $\\alpha = 0.0003, n = 800$ ---> trainerror: 97.87, testerror: 89.56\n",
    "* $\\alpha = 0.0003, n = 1600$ ---> trainerror: 98.4, testerror: 89.56\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
